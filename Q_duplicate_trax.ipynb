{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jv7Y4hXwt0j"
      },
      "source": [
        "#  Question duplicates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sF9Hqzgwt0l"
      },
      "source": [
        "## Importing and Loading the Data\n",
        "\n",
        "I will be using the Quora question answer dataset to build a model that could identify similar questions. This is a particularly useful task because we don't want to have several versions of the same question posted. The data has been already labelled here. By running the cell below we can import some of the useful and important packages for this project.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "zdACgs491cs2",
        "outputId": "b31042ef-845b-46b8-c783-185e96b135f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "from trax.fastmath import numpy as fastnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rnd\n",
        "\n",
        "# set random seeds\n",
        "trax.supervised.trainer_lib.init_random_number_generators(34)\n",
        "rnd.seed(34)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GYhQRMspitx"
      },
      "source": [
        "**In this project Trax's numpy is referred to as `fastnp`, while the regular numpy is referred to as `np`**\n",
        "\n",
        "In the cell below, I am loading the dataset and doing some essential preprocessing required.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "sXWBVGWnpity",
        "outputId": "afa90d4d-fed7-43b8-bcba-48c95d600ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(404351, 6)\n",
            "Number of question pairs:  404351\n",
            "the number of duplicates: 149306\n",
            "example of duplicates: id                                                              5\n",
            "qid1                                                           11\n",
            "qid2                                                           12\n",
            "question1       Astrology: I am a Capricorn Sun Cap moon and c...\n",
            "question2       I'm a triple Capricorn (Sun, Moon and ascendan...\n",
            "is_duplicate                                                    1\n",
            "Name: 5, dtype: object\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  qid2                                          question1  \\\n",
              "0   0     1     2  What is the step by step guide to invest in sh...   \n",
              "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2   2     5     6  How can I increase the speed of my internet co...   \n",
              "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
              "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
              "\n",
              "                                           question2  is_duplicate  \n",
              "0  What is the step by step guide to invest in sh...             0  \n",
              "1  What would happen if the Indian government sto...             0  \n",
              "2  How can Internet speed be increased by hacking...             0  \n",
              "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
              "4            Which fish would survive in salt water?             0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"questions.csv\")\n",
        "N=len(data)\n",
        "print(data.shape)\n",
        "print('Number of question pairs: ', N)\n",
        "\n",
        "print('the number of duplicates:',np.sum(data['is_duplicate']==1))\n",
        "print('example of duplicates:',data[data['is_duplicate']==1].iloc[0])\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkSQTu7Ypit0"
      },
      "source": [
        "We first split the data into a train and test set. The test set will be used later to evaluate our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "z00A7vEMpit1",
        "outputId": "c12ae7e8-a959-4f56-aa29-6ad34abc1c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set: 300000 Test set: 10240\n"
          ]
        }
      ],
      "source": [
        "N_train = 300000\n",
        "N_test  = 10*1024\n",
        "data_train = data[:N_train]\n",
        "data_test  = data[N_train:N_train+N_test]\n",
        "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n",
        "del(data) # remove to free memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbqIRRyEpit4"
      },
      "source": [
        "We are selecting only the questio pairs that are duplicate to train the model. \n",
        "We build two batches as input for the Siamese network and we assume that question $q1_i$ (question $i$ in the first batch) is a duplicate of $q2_i$ (question $i$ in the second batch), but all other questions in the second batch are not duplicates of $q1_i$.  \n",
        "The test set uses the original pairs of questions and the status describing if the questions are duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "Xi_TwXxxpit4",
        "outputId": "f146046f-9c0d-4d8a-ecf8-8d6a4a5371f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of duplicate questions:  111486\n",
            "indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
          ]
        }
      ],
      "source": [
        "td_index = (data_train['is_duplicate'] == 1).to_numpy()\n",
        "td_index = [i for i, x in enumerate(td_index) if x] \n",
        "print('number of duplicate questions: ', len(td_index))\n",
        "print('indexes of first ten duplicate questions:', td_index[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "3I9oXSsKpit7",
        "outputId": "6f6bd3a1-219f-4fb3-a524-450c38bf44ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How can I be a good geologist?\n",
            "What should I do to be a great geologist?\n",
            "is_duplicate:  1\n"
          ]
        }
      ],
      "source": [
        "print(data_train['question1'][7])  #  Example of question duplicates (first one in data)\n",
        "print(data_train['question2'][7])\n",
        "print('is_duplicate: ', data_train['is_duplicate'][7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHpZO58Dss_v"
      },
      "outputs": [],
      "source": [
        "Q1_train_words = np.array(data_train['question1'][td_index])\n",
        "Q2_train_words = np.array(data_train['question2'][td_index])\n",
        "\n",
        "Q1_test_words = np.array(data_test['question1'])\n",
        "Q2_test_words = np.array(data_test['question2'])\n",
        "y_test  = np.array(data_test['is_duplicate'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5vBkxunpiuB"
      },
      "source": [
        "\n",
        "\n",
        "Here I only took duplicated questions for training the model, because the data generator will produce batches $([q1_1, q1_2, q1_3, ...]$, $[q2_1, q2_2,q2_3, ...])$  where $q1_i$ and $q2_k$ are duplicate if and only if $i = k$.\n",
        "\n",
        "<br>Let's print to see what the data looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "joyrS1XEpLWn",
        "outputId": "3257cde7-3164-40d9-910e-fa91eae917a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING QUESTIONS:\n",
            "\n",
            "Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
            "Question 2:  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? \n",
            "\n",
            "Question 1:  What would a Trump presidency mean for current international master’s students on an F1 visa?\n",
            "Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? \n",
            "\n",
            "TESTING QUESTIONS:\n",
            "\n",
            "Question 1:  How do I prepare for interviews for cse?\n",
            "Question 2:  What is the best way to prepare for cse? \n",
            "\n",
            "is_duplicate = 0 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('TRAINING QUESTIONS:\\n')\n",
        "print('Question 1: ', Q1_train_words[0])\n",
        "print('Question 2: ', Q2_train_words[0], '\\n')\n",
        "print('Question 1: ', Q1_train_words[5])\n",
        "print('Question 2: ', Q2_train_words[5], '\\n')\n",
        "\n",
        "print('TESTING QUESTIONS:\\n')\n",
        "print('Question 1: ', Q1_test_words[0])\n",
        "print('Question 2: ', Q2_test_words[0], '\\n')\n",
        "print('is_duplicate =', y_test[0], '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC_BZU3XpiuF"
      },
      "source": [
        "Here I am tokenizing the questions using `ntlk.word_tokenize` and also building a python default dictionary, which later assigns values 0 to all the Out Of Vocabuary (OOV) words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbCoIgLQpiuF",
        "outputId": "2d7c5202-6788-461e-d059-121625efd7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(111486,)\n"
          ]
        }
      ],
      "source": [
        "#create arrays\n",
        "Q1_train = np.empty_like(Q1_train_words)\n",
        "Q2_train = np.empty_like(Q2_train_words)\n",
        "\n",
        "Q1_test = np.empty_like(Q1_test_words)\n",
        "Q2_test = np.empty_like(Q2_test_words)\n",
        "print(Q1_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "m9ZmfpGWpiuI",
        "outputId": "d2995c9a-92b4-4892-d34b-c77b94b27134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The length of the vocabulary is:  36268\n"
          ]
        }
      ],
      "source": [
        "# Building the vocabulary with the train set         (this might take a minute)\n",
        "from collections import defaultdict\n",
        "\n",
        "vocab = defaultdict(lambda: 0)\n",
        "vocab['<PAD>'] = 1\n",
        "\n",
        "for idx in range(len(Q1_train_words)):\n",
        "    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])\n",
        "    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])\n",
        "    q = Q1_train[idx] + Q2_train[idx]\n",
        "    for word in q:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab) + 1\n",
        "print('The length of the vocabulary is: ', len(vocab))\n",
        "#print(Q1_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "TTMRF8eZpiuK",
        "outputId": "f81d4dc1-7cf9-4476-a454-467b54fe4dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(vocab['<PAD>'])\n",
        "print(vocab['Astrology'])\n",
        "print(vocab['Astronomy'])  #not in vocabulary, returns 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sDs36m81g6f"
      },
      "outputs": [],
      "source": [
        "for idx in range(len(Q1_test_words)): \n",
        "    Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])\n",
        "    Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "3QgGE9KlpiuP",
        "outputId": "19c3cf93-cf0d-4f8f-da99-e75481f16599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set has reduced to:  111486\n",
            "Test set length:  10240\n"
          ]
        }
      ],
      "source": [
        "print('Train set has reduced to: ', len(Q1_train) ) \n",
        "print('Test set length: ', len(Q1_test) ) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDcxEmX31y3d"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "### Converting a question to a tensor\n",
        "\n",
        "Coverting every question to a tensor, or an array of numbers, using the vocabulary built above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOhNa-sapiuS"
      },
      "outputs": [],
      "source": [
        "# Converting questions to array of integers\n",
        "for i in range(len(Q1_train)):\n",
        "    Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n",
        "    Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n",
        "\n",
        "        \n",
        "for i in range(len(Q1_test)):\n",
        "    Q1_test[i] = [vocab[word] for word in Q1_test[i]]\n",
        "    Q2_test[i] = [vocab[word] for word in Q2_test[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "Dpawm38dpiuU",
        "outputId": "ef1aa65b-c89b-46f9-a9cf-f73748f1ee56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first question in the train set:\n",
            "\n",
            "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
            "\n",
            "encoded version:\n",
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] \n",
            "\n",
            "first question in the test set:\n",
            "\n",
            "How do I prepare for interviews for cse? \n",
            "\n",
            "encoded version:\n",
            "[32, 38, 4, 107, 65, 1015, 65, 11509, 21]\n"
          ]
        }
      ],
      "source": [
        "print('first question in the train set:\\n')\n",
        "print(Q1_train_words[0], '\\n') \n",
        "print('encoded version:')\n",
        "print(Q1_train[0],'\\n')\n",
        "\n",
        "print('first question in the test set:\\n')\n",
        "print(Q1_test_words[0], '\\n')\n",
        "print('encoded version:')\n",
        "print(Q1_test[0]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuggGPaQpiuY"
      },
      "source": [
        "I now split your train set into a training/validation set so that it can be used to train and evaluate the Siamese Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "BmhrWPtgpiuY",
        "outputId": "7272fb74-79e6-499a-ce95-d11b9edcd64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate questions:  111486\n",
            "The length of the training set is:   89188\n",
            "The length of the validation set is:  22298\n"
          ]
        }
      ],
      "source": [
        "# Splitting the data\n",
        "cut_off = int(len(Q1_train)*.8)\n",
        "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
        "val_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]\n",
        "print('Number of duplicate questions: ', len(Q1_train))\n",
        "print(\"The length of the training set is:  \", len(train_Q1))\n",
        "print(\"The length of the validation set is: \", len(val_Q1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFOR19cX2TQs"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "The commmand `next(data_generator)` returns the next batch. This iterator returns the data in the format that we can drectly use in the model when computing the feed-forward part of the algorithm. This iterator returns a pair of array of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibchgos48MtA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n",
        "\n",
        "\n",
        "    input1 = []\n",
        "    input2 = []\n",
        "    idx = 0\n",
        "    len_q = len(Q1)\n",
        "    question_indexes = [*range(len_q)]\n",
        "    \n",
        "    if shuffle:\n",
        "        rnd.shuffle(question_indexes)\n",
        "    \n",
        "\n",
        "    while True:\n",
        "        if idx >= len_q:\n",
        "\n",
        "            idx = 0\n",
        "            if shuffle:\n",
        "                rnd.shuffle(question_indexes)\n",
        "\n",
        "        q1 = Q1[question_indexes[idx]]\n",
        "        q2 = Q2[question_indexes[idx]]\n",
        "        \n",
        "        # increment idx by 1\n",
        "        idx += 1\n",
        "        # append q1\n",
        "        input1.append(q1)\n",
        "        # append q2\n",
        "        input2.append(q2)\n",
        "       \n",
        "        if len(input1) == batch_size:\n",
        "            max_len = max(max([len(q) for q in input1]),max([len(q) for q in input2]))\n",
        "            max_len = 2**int(np.ceil(np.log2(max_len)))\n",
        "            b1 = []\n",
        "            b2 = []\n",
        "            for q1, q2 in zip(input1, input2):\n",
        "                # add [pad] to q1 until it reaches max_len\n",
        "                q1 = q1+(max_len-len(q1))*[pad]\n",
        "                # add [pad] to q2 until it reaches max_len\n",
        "                q2 = q2+(max_len-len(q2))*[pad]\n",
        "                # append q1\n",
        "                b1.append(q1)\n",
        "                # append q2\n",
        "                b2.append(q2)\n",
        "            # use b1 and b2\n",
        "            yield np.array(b1), np.array(b2)\n",
        "            # reset the batches\n",
        "            input1, input2 = [], []  # reset the batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZFZeBPnW8Mlb",
        "outputId": "7a31cd19-55dc-4b97-f288-6c59c6a34b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First questions  :  \n",
            " [[  30   87   78  134 2132 1981   28   78  594   21    1    1    1    1\n",
            "     1    1]\n",
            " [  30   55   78 3541 1460   28   56  253   21    1    1    1    1    1\n",
            "     1    1]] \n",
            "\n",
            "Second questions :  \n",
            " [[  30  156   78  134 2132 9508   21    1    1    1    1    1    1    1\n",
            "     1    1]\n",
            " [  30  156   78 3541 1460  131   56  253   21    1    1    1    1    1\n",
            "     1    1]]\n"
          ]
        }
      ],
      "source": [
        "batch_size = 2\n",
        "res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))\n",
        "print(\"First questions  : \",'\\n', res1, '\\n')\n",
        "print(\"Second questions : \",'\\n', res2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmZRBoaMwt0w"
      },
      "source": [
        "## Defining Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hww76f8_wt0x"
      },
      "outputs": [],
      "source": [
        "def Siamese(vocab_size=len(vocab), d_model=128, mode='train'):\n",
        "\n",
        "\n",
        "    def normalize(x):  # normalizes the vectors to have L2 norm 1\n",
        "        return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n",
        "    \n",
        "\n",
        "    q_processor = tl.Serial(  # Processor will run on Q1 and Q2.\n",
        "        tl.Embedding(vocab_size,d_model), # Embedding layer\n",
        "        tl.LSTM(n_units=d_model), # LSTM layer\n",
        "        tl.Mean(axis=1), # Mean over columns\n",
        "        tl.Fn('normalize',lambda x:normalize(x))  # Apply normalize function\n",
        "    )  # Returns one vector of shape [batch_size, d_model].\n",
        "    \n",
        "    \n",
        "    # Run on Q1 and Q2 in parallel.\n",
        "    model = tl.Parallel(q_processor, q_processor)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es2gfwZypiul"
      },
      "source": [
        "Setup the Siamese network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "kvQ_jf52-JAn",
        "outputId": "d409460d-2ffb-4ae6-8745-ddcfa1d892ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parallel_in2_out2[\n",
            "  Serial[\n",
            "    Embedding_41699_128\n",
            "    LSTM_128\n",
            "    Mean\n",
            "    normalize\n",
            "  ]\n",
            "  Serial[\n",
            "    Embedding_41699_128\n",
            "    LSTM_128\n",
            "    Mean\n",
            "    normalize\n",
            "  ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# check your model\n",
        "model = Siamese()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVo1Gvripiuo"
      },
      "source": [
        "Implementing Triplet Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJM8EQiopiuv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def TripletLossFn(v1, v2, margin=0.25):\n",
        "\n",
        "    # use fastnp to take the dot product of the two batches (don't forget to transpose the second argument)\n",
        "    scores = fastnp.matmul(v1,v2.T)  # pairwise cosine sim\n",
        "    # calculate new batch size\n",
        "    batch_size = len(scores)\n",
        "    # use fastnp to grab all postive `diagonal` entries in `scores`\n",
        "    positive = fastnp.diagonal(scores)  # the positive ones (duplicates)\n",
        "    # multiply `fastnp.eye(batch_size)` with 2.0 and subtract it out of `scores`\n",
        "    negative_without_positive = scores-fastnp.eye(batch_size)*2.0\n",
        "    # take the row by row `max` of `negative_without_positive`. \n",
        "    closest_negative = negative_without_positive.max(axis=1)\n",
        "    # subtract `fastnp.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`\n",
        "    negative_zero_on_duplicate = (1-fastnp.eye(batch_size))*scores\n",
        "    # use `fastnp.sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)` \n",
        "    mean_negative = fastnp.sum(negative_zero_on_duplicate,axis=1)/(batch_size-1)\n",
        "    # compute `fastnp.maximum` among 0.0 and `A`\n",
        "    # A = subtract `positive` from `margin` and add `closest_negative` \n",
        "    triplet_loss1 = fastnp.maximum(-positive+mean_negative+margin,0)\n",
        "    # compute `fastnp.maximum` among 0.0 and `B`\n",
        "    # B = subtract `positive` from `margin` and add `mean_negative`\n",
        "    triplet_loss2 = fastnp.maximum(-positive+closest_negative+margin,0)\n",
        "    # add the two losses together and take the `fastnp.mean` of it\n",
        "    triplet_loss = fastnp.mean(triplet_loss1+triplet_loss2)\n",
        "\n",
        "    return triplet_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IJCL_A5MI24",
        "outputId": "5dd58b08-a24f-44d3-fab5-f4938992a295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Triplet Loss: 0.5\n"
          ]
        }
      ],
      "source": [
        "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
        "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
        "TripletLossFn(v2,v1)\n",
        "print(\"Triplet Loss:\", TripletLossFn(v2,v1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r974ozuHYAom"
      },
      "source": [
        "To make a layer out of a function with no trainable variables, use `tl.Fn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wilKIA6oMI26"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "def TripletLoss(margin=0.25):\n",
        "    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
        "    return tl.Fn('TripletLoss', triplet_loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsvjaCQ6wt02"
      },
      "source": [
        "\n",
        "\n",
        "# Training\n",
        "We will define the inouts using the data generator built above. the lambda function acts as a seed to remember the last batch that was given. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "iPk7gh-nzCBg",
        "outputId": "a2e8525d-f89a-4d9d-c0d6-bd7406f0246a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_Q1.shape  (89188,)\n",
            "val_Q1.shape    (22298,)\n"
          ]
        }
      ],
      "source": [
        "batch_size = 256\n",
        "train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab['<PAD>'])\n",
        "val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab['<PAD>'])\n",
        "print('train_Q1.shape ', train_Q1.shape)\n",
        "print('val_Q1.shape   ', val_Q1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kbtfz4T_m7x"
      },
      "outputs": [],
      "source": [
        "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
        "\n",
        "def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir='model/'):\n",
        "\n",
        "    output_dir = os.path.expanduser(output_dir)\n",
        "\n",
        "\n",
        "\n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=train_generator,       # Use generator (train)\n",
        "        loss_layer=TripletLoss(),         # Use triplet loss. Don't forget to instantiate this object\n",
        "        optimizer=trax.optimizers.Adam(0.01),          # Don't forget to add the learning rate parameter\n",
        "        lr_schedule=lr_schedule, # Use Trax multifactor schedule function\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=val_generator,       # Use generator (val)\n",
        "        metrics=[TripletLoss()],          # Use triplet loss. Don't forget to instantiate this object\n",
        "    )\n",
        "    \n",
        "\n",
        "    training_loop = training.Loop(Siamese(),\n",
        "                                  train_task,\n",
        "                                  eval_task=eval_task,\n",
        "                                  output_dir=output_dir)\n",
        "\n",
        "    return training_loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "-3KXjmBo_6Xa",
        "outputId": "9d57f731-1534-4218-e744-783359d5cd19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step      1: train TripletLoss |  0.49926734\n",
            "Step      1: eval  TripletLoss |  0.49950904\n"
          ]
        }
      ],
      "source": [
        "train_steps = 5\n",
        "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
        "training_loop.run(train_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJSm4mHBMI28"
      },
      "source": [
        "The model was only trained for 5 steps due to the constraints of this environment. For the rest of the assignment you will be using a pretrained model but now you should understand how the training can be done using Trax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abKPe7d4wt1C"
      },
      "source": [
        "Evaluating the Siamese Network "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OtmlEuOwt1D",
        "outputId": "aed471fa-7f3f-453b-e720-66ed3851cc73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parallel_in2_out2[\n",
              "  Serial[\n",
              "    Embedding_41699_128\n",
              "    LSTM_128\n",
              "    Mean\n",
              "    normalize\n",
              "  ]\n",
              "  Serial[\n",
              "    Embedding_41699_128\n",
              "    LSTM_128\n",
              "    Mean\n",
              "    normalize\n",
              "  ]\n",
              "]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading in the saved model\n",
        "model = Siamese()\n",
        "model.init_from_file('model.pkl.gz')\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-h6ZH507fUm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def classify(test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=64):\n",
        "\n",
        "    accuracy = 0\n",
        " \n",
        "    for i in range(0, len(test_Q1), batch_size):\n",
        "        # Call the data generator (built in Ex 01) with shuffle=False using next()\n",
        "        # use batch size chuncks of questions as Q1 & Q2 arguments of the data generator. e.g x[i:i + batch_size]\n",
        "        q1, q2 = next(data_generator(test_Q1[i:i+batch_size],test_Q2[i:i+batch_size],batch_size,vocab['<PAD>'],shuffle=False))\n",
        "        # use batch size chuncks of actual output targets (same syntax as example above)\n",
        "        y_test = y[i:i+batch_size]\n",
        "        # Call the model\n",
        "        v1, v2 = model((q1,q2))\n",
        "\n",
        "        for j in range(batch_size):\n",
        "            # take dot product to compute cos similarity of each pair of entries, v1[j], v2[j]\n",
        "            # don't forget to transpose the second argument\n",
        "            d = np.dot(v1[j],v2[j])\n",
        "            # is d greater than the threshold?\n",
        "            res = d>threshold\n",
        "            # increment accurancy if y_test is equal `res`\n",
        "            accuracy += (res==y_test[j])\n",
        "    # compute accuracy using accuracy and total length of test questions\n",
        "    accuracy = accuracy/len(test_Q1)\n",
        "    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "yeQjHxkfpivH",
        "outputId": "103b8449-896f-403d-f011-583df70afdae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.69091796875\n"
          ]
        }
      ],
      "source": [
        "# this takes around 1 minute\n",
        "accuracy = classify(Q1_test,Q2_test, y_test, 0.7, model, vocab, batch_size = 512) \n",
        "print(\"Accuracy\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-STC44Ywt1I"
      },
      "source": [
        "<a name='5'></a>\n",
        "\n",
        "# Testing with your own questions\n",
        "\n",
        "In this section you will test the model with your own questions. You will write a function `predict` which takes two questions as input and returns $1$ or $0$ depending on whether the question pair is a duplicate or not.   \n",
        "\n",
        "But first, we build a reverse vocabulary that allows to map encoded questions back to words: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21h3Y0FNpivK"
      },
      "source": [
        "\n",
        "\n",
        "`Predict` function below takes in two questions, the model, and the vocabulary and returns whether the questins are duplicates (1) or not duplicates (0) given a similarity method. \n",
        "\n",
        "Workflow :\n",
        "* Tokenize the question using `nltk.word_tokenize`\n",
        "* Create Q1, Q2 by encoding the questions as a list of numbers using vocab\n",
        "* pad Q1, Q2 with next(data_generator([Q1], [Q2], 1, vocab['<PAD>']))\n",
        "* use model() to create v1, v2\n",
        "* compute the cosine similarity (dot product) of v1, v2\n",
        "* compute res by comparing d to the threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg0wQ8qhpivL"
      },
      "outputs": [],
      "source": [
        "def predict(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False):\n",
        "\n",
        "\n",
        "    # use `nltk` word tokenize function to tokenize\n",
        "    q1 = nltk.word_tokenize(question1)  # tokenize\n",
        "    q2 = nltk.word_tokenize(question2)  # tokenize\n",
        "    Q1, Q2 = [vocab[x] for x in q1], [vocab[x] for x in q2]\n",
        "\n",
        "    Q1, Q2 = next(data_generator([Q1],[Q2],1,vocab['<PAD>'],shuffle=False))\n",
        "    # Call the model\n",
        "    v1, v2 = model((Q1,Q2))\n",
        "    # take dot product to compute cos similarity of each pair of entries, v1, v2\n",
        "    # don't forget to transpose the second argument\n",
        "    d = fastnp.dot(v1,v2.T)\n",
        "    # is d greater than the threshold?\n",
        "    res = d>threshold\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    if(verbose):\n",
        "        print(\"Q1  = \", question1, \"\\nQ2  = \", question2)\n",
        "        print(\"d   = \", d)\n",
        "        print(\"res = \", res)\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "Raojyhw3z7HE",
        "outputId": "b0907aaf-63c0-448d-99b0-012359381a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1  =  When will I see you? \n",
            "Q2  =  When can I see you again?\n",
            "d   =  [[0.8811324]]\n",
            "res =  [[ True]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeviceArray([[ True]], dtype=bool)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feel free to try with your own questions\n",
        "question1 = \"When will I see you?\"\n",
        "question2 = \"When can I see you again?\"\n",
        "# 1 means it is duplicated, 0 otherwise\n",
        "predict(question1 , question2, 0.7, model, vocab, verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "DZccIQ_lpivQ",
        "outputId": "3ed0af7e-5d44-4eb3-cebe-d6f74abe3e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1  =  Do they enjoy eating the dessert? \n",
            "Q2  =  Do they like hiking in the desert?\n",
            "d   =  [[0.477536]]\n",
            "res =  [[False]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeviceArray([[False]], dtype=bool)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feel free to try with your own questions\n",
        "question1 = \"Do they enjoy eating the dessert?\"\n",
        "question2 = \"Do they like hiking in the desert?\"\n",
        "# 1 means it is duplicated, 0 otherwise\n",
        "predict(question1 , question2, 0.7, model, vocab, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zpj2RUkcMI3C",
        "outputId": "bc570e3c-c2d1-41cd-edc7-849756807fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1  =  she looks overweight? \n",
            "Q2  =  she looks smart?\n",
            "d   =  [[0.7437502]]\n",
            "res =  [[ True]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeviceArray([[ True]], dtype=bool)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feel free to try with your own questions\n",
        "question1 = \"she looks overweight?\"\n",
        "question2 = \"she looks smart?\"\n",
        "# 1 means it is duplicated, 0 otherwise\n",
        "predict(question1 , question2, 0.7, model, vocab, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAfV3l5Zwt1L"
      },
      "source": [
        "We can see that the Siamese network is capable of catching complicated structures. Concretely it can identify question duplicates although the questions do not have many words in common. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsE8tdTLwt1M"
      },
      "source": [
        "<a name='6'></a>\n",
        "\n",
        "###  <span style=\"color:blue\"> On Siamese networks </span>\n",
        "\n",
        "Siamese networks are important and useful. Many times there are several questions that are already asked in quora, or other platforms and we can use Siamese networks to avoid question duplicates. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "coursera": {
      "schema_names": [
        "NLPC3-4A"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}